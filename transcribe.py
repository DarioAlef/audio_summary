import whisper
import torch
import os
import warnings
from pathlib import Path
import math

warnings.filterwarnings("ignore", message=".*Triton kernels.*")

def verificar_gpu():
    if torch.cuda.is_available():
        gpu_name = torch.cuda.get_device_name(0)
        cuda_version = torch.version.cuda
        print(f"‚úÖ GPU detectada: {gpu_name}")
        print(f"üî• VRAM dispon√≠vel: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB")
        print(f"‚ö° CUDA version: {cuda_version}")
        return True
    else:
        print("‚ö†Ô∏è  GPU n√£o detectada. Usando CPU.")
        return False

def transcrever_audio_segmentado(caminho_audio, modelo="base", usar_gpu=True, tamanho_segmento=1800):
    """
    Transcreve √°udio longo dividindo em segmentos para evitar repeti√ß√µes.
    tamanho_segmento: dura√ß√£o em segundos (padr√£o: 30 minutos)
    """
    gpu_disponivel = verificar_gpu() and usar_gpu
    device = "cuda" if gpu_disponivel else "cpu"
    
    print(f"Carregando modelo Whisper '{modelo}' no {device.upper()}...")
    
    if gpu_disponivel:
        torch.backends.cudnn.benchmark = True
        torch.backends.cuda.matmul.allow_tf32 = True
    
    model = whisper.load_model(modelo, device=device)
    
    print(f"üîÑ Processando √°udio em segmentos de {tamanho_segmento//60} minutos para evitar repeti√ß√µes...")
    
    # Carrega o √°udio completo para obter informa√ß√µes
    audio = whisper.load_audio(caminho_audio)
    duracao_total = len(audio) / whisper.audio.SAMPLE_RATE
    
    print(f"üìä Dura√ß√£o total do √°udio: {duracao_total/60:.1f} minutos")
    
    # Calcula n√∫mero de segmentos
    num_segmentos = math.ceil(duracao_total / tamanho_segmento)
    print(f"üî¢ Dividindo em {num_segmentos} segmentos")
    
    texto_completo = ""
    
    for i in range(num_segmentos):
        inicio = i * tamanho_segmento * whisper.audio.SAMPLE_RATE
        fim = min((i + 1) * tamanho_segmento * whisper.audio.SAMPLE_RATE, len(audio))
        
        segmento_audio = audio[int(inicio):int(fim)]
        
        print(f"\nüéØ Processando segmento {i+1}/{num_segmentos} ({inicio/whisper.audio.SAMPLE_RATE/60:.1f}-{fim/whisper.audio.SAMPLE_RATE/60:.1f} min)")
        
        # Configura√ß√µes anti-repeti√ß√£o para cada segmento
        result = model.transcribe(
            segmento_audio,
            language="pt",
            verbose=False,  # Reduz spam no console
            fp16=gpu_disponivel,
            temperature=0.2,
            beam_size=1,
            best_of=1,
            word_timestamps=False,
            no_speech_threshold=0.6,
            logprob_threshold=-1.0,
            compression_ratio_threshold=2.4,
            condition_on_previous_text=False,
            hallucination_silence_threshold=3.0
        )
        
        # Adiciona uma quebra entre segmentos
        if i > 0:
            texto_completo += "\n\n"
        texto_completo += result["text"]
        
        print(f"‚úÖ Segmento {i+1} conclu√≠do: {len(result['text'])} caracteres")
        
        # Limpa cache da GPU entre segmentos
        if gpu_disponivel:
            torch.cuda.empty_cache()
    
    return texto_completo

def transcrever_audio_longo(caminho_audio, modelo="base", usar_gpu=True, usar_segmentacao=True):
    """
    Fun√ß√£o principal de transcri√ß√£o com op√ß√£o de segmenta√ß√£o autom√°tica.
    usar_segmentacao: Se True, divide √°udios longos em segmentos (recomendado)
    """
    if usar_segmentacao:
        # Usa a nova fun√ß√£o segmentada para evitar repeti√ß√µes
        texto = transcrever_audio_segmentado(caminho_audio, modelo, usar_gpu)
    else:
        # Fun√ß√£o original (pode ter problemas com √°udios longos)
        gpu_disponivel = verificar_gpu() and usar_gpu
        device = "cuda" if gpu_disponivel else "cpu"
        
        print(f"Carregando modelo Whisper '{modelo}' no {device.upper()}...")
        
        if gpu_disponivel:
            torch.backends.cudnn.benchmark = True
            torch.backends.cuda.matmul.allow_tf32 = True
        
        model = whisper.load_model(modelo, device=device)
        
        print(f"Iniciando transcri√ß√£o de: {caminho_audio}")
        if gpu_disponivel:
            print("üöÄ Usando GPU - velocidade ~10x mais r√°pida!")
            print("‚ö†Ô∏è  Avisos sobre Triton s√£o normais e n√£o afetam o desempenho")
        else:
            print("‚ö†Ô∏è  Para √°udios de 2h+, isso pode demorar 30-60 minutos...")
        
        # Configura√ß√µes para evitar repeti√ß√µes em √°udios longos
        result = model.transcribe(
            caminho_audio,
            language="pt",              
            verbose=True,
            fp16=gpu_disponivel,        
            temperature=0.2,            # Aumenta diversidade, evita repeti√ß√µes
            beam_size=1,                # Reduz para evitar loops
            best_of=1,                  # Reduz para evitar loops
            word_timestamps=False,
            no_speech_threshold=0.6,    # Detecta melhor sil√™ncios
            logprob_threshold=-1.0,     # Filtra tokens com baixa confian√ßa
            compression_ratio_threshold=2.4,  # Detecta repeti√ß√µes
            condition_on_previous_text=False,  # Evita depend√™ncia de texto anterior
            hallucination_silence_threshold=3.0  # Detecta alucina√ß√µes em sil√™ncios
        )
        
        if gpu_disponivel:
            torch.cuda.empty_cache()
        
        texto = result["text"]
    
    # Salva o resultado
    output_dir = Path("./texto")
    output_dir.mkdir(exist_ok=True)
    
    output_path = output_dir / "texto_gerado.md"
    with open(output_path, 'w', encoding='utf-8') as f:
        f.write("# Transcri√ß√£o do √Åudio\n\n")
        f.write(texto)
    
    print(f"\n‚úÖ Transcri√ß√£o salva em: {output_path}")
    print(f"üìä Texto gerado: {len(texto)} caracteres")
    
    return texto

def main():
    print("=== TRANSCRI√á√ÉO DE √ÅUDIO COM WHISPER ===\n")
    
    gpu_disponivel = verificar_gpu()
    
    while True:
        caminho_audio = input("Caminho do arquivo de √°udio: ").strip().strip('"')
        if os.path.exists(caminho_audio):
            break
        print("‚ùå Arquivo n√£o encontrado. Tente novamente.")
    
    print("\nEscolha o modelo:")
    if gpu_disponivel:
        print("üöÄ GPU DETECTADA - Tempos para √°udio de 2h:")
        print("1. tiny   - ~2 minutos")
        print("2. base   - ~6 minutos (recomendado)")
        print("3. small  - ~12 minutos")
        print("4. medium - ~20 minutos")
        print("5. large  - ~40 minutos (m√°xima qualidade)")
        print("\n‚ö†Ô∏è  Para sua GTX 1650 (4GB), recomendo 'base' ou 'small'")
    else:
        print("üêå CPU - Tempos para √°udio de 2h:")
        print("1. tiny   - ~20 minutos")
        print("2. base   - ~60 minutos")
        print("3. small  - ~120 minutos")
        print("4. medium - ~240 minutos")
        print("5. large  - ~480 minutos")
    
    escolha = input("Escolha (1-5, padr√£o=2): ").strip() or "2"
    modelos = {"1": "tiny", "2": "base", "3": "small", "4": "medium", "5": "large"}
    modelo = modelos.get(escolha, "base")
    
    if modelo == "large" and gpu_disponivel:
        print("‚ö†Ô∏è  ATEN√á√ÉO: Modelo 'large' pode ser lento na GTX 1650")
        continuar = input("Continuar mesmo assim? (s/N): ").strip().lower()
        if continuar != 's':
            modelo = "base"
            print("‚úÖ Usando modelo 'base' (mais adequado para sua GPU)")
    
    # Nova op√ß√£o para escolher m√©todo de transcri√ß√£o
    print("\nüéØ M√©todo de transcri√ß√£o:")
    print("1. Segmentado (recomendado para √°udios >30min) - Evita repeti√ß√µes")
    print("2. Completo (mais r√°pido, mas pode repetir em √°udios longos)")
    
    metodo = input("Escolha (1-2, padr√£o=1): ").strip() or "1"
    usar_segmentacao = metodo == "1"
    
    if usar_segmentacao:
        print("‚úÖ Usando m√©todo segmentado - divide o √°udio para evitar repeti√ß√µes")
    else:
        print("‚ö†Ô∏è  M√©todo completo - pode repetir frases em √°udios muito longos")
    
    try:
        transcrever_audio_longo(caminho_audio, modelo, gpu_disponivel, usar_segmentacao)
        print("\nüéâ Transcri√ß√£o conclu√≠da! Execute summary_text.py para gerar o resumo.")
    except Exception as e:
        print(f"‚ùå Erro na transcri√ß√£o: {e}")
        if "out of memory" in str(e).lower():
            print("üí° Tente um modelo menor ou feche outros programas que usam GPU")

if __name__ == "__main__":
    main()